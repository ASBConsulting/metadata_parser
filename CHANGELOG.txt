0.6.15
	- fixed README which used old api in the example

0.6.14
	- there was a typo and another bug that passed some tests on BeautifulSoup parsing.  they have been fixed.  todo- migrate tests to public repo

0.6.13
	- trying to integrate a "safe read"

0.6.12
	- now passing "stream=True" to requests.get.  this will fetch the headers first, before looping through the response.  we can avoid many issues with this approach

0.6.11
	- now correctly validating urls with ports. had to restructure a lot of the url validation

0.6.10
	- changed how some nodes are inspected. this should lead to fewer errors

0.6.9
	- added a new method `get_metadata_link()`, which applies link transformations to a metadata in an attempt to ensure a valid link

0.6.8
	- added a kwarg `requests_timeout` to proxy a timeout value to `requests.get()`

0.6.7
	- added a lockdown to `is_parsed_valid_url` titled `http_only` -- requires http/https for the scheme

0.6.6
	- protecting against bad doctypes, like nasa.gov
	-- added `force_doctype` to __init__.  defaults to False. this will change the doctype to get around to bs4/lxml issues
	-- this is defaulted to False.

0.6.5
	- keeping the parsed BS4 document; a user may wish to perform further operations on it.
	-- `MetadataParser.soup` attribute holds BS4 document

0.6.4
	- flake8 fixes. purely cosmetic.

0.6.3
	- no changes.  `sdist upload` was picking up a reference file that wasn't in github; that file killed the distribution install

0.6.2
	- formatting fixes via flake8

0.6.1
	- Lightweight, but functional, url validation
	-- new 'init' argument (defaults to True) : `require_public_netloc`
	-- this will ensure a url's hostname/netloc is either an IPV4 or "public DNS" name
	-- if the url is entirely numeric, requires it to be IPV4
	-- if the url is alphanumeric, requires a TLD + Domain ( exception is "localhost" )
	-- this is NOT RFC compliant, but designed for "Real Life" use cases.

0.6.0
	- Several fixes to improve support of canonical and absolute urls
	-- replaced REGEX parsing of urls with `urlparse` parsing and inspection; too many edge cases got in
	-- refactored `MediaParser.absolute_url` , now proxies a call to new function `url_to_absolute_url`
	-- refactored `MediaParser.get_discrete_url` , now cleaner and leaner.
	-- refactored how some tests run, so there is cleaner output


0.5.8
	- trying to fix some issues with distribution

0.5.7
	- trying to parse unparsable pages was creating an error
	-- `MetadataParser.init` now accepts `only_parse_file_extensions` -- list of the only file extensions to parse
	-- `MetadataParser.init` now accepts `force_parse_invalid_content_type` -- forces to parse invalid content
	-- `MetadataParser.fetch_url` will only parse "text/html" content by default

0.5.6
	- trying to ensure we return a valid url in get_discrete_url()
	- adding in some proper unit tests; migrating from the private demo's slowly ( the private demo's hit a lot of internal files and public urls ; wouldn't be proper to make these public )
	- setting `self.url_actual = url` on __init__. this will get overridden on a `fetch`, but allows for a fallback on html docs passed through


0.5.5
	- Dropped BS3 support
	- test Python3 support ( support added by Paul Bonser [ https://github.com/pib ] )


0.5.4
	- Pull Request - https://github.com/jvanasco/metadata_parser/pull/1
		Credit to Paul Bonser [ https://github.com/pib ]

0.5.3
	- added a few `.strip()` calls to clean up metadata values

0.5.2
	- fixed an issue on html title parsing.  the old method incorrectly regexed on a BS4 tag, not tag contents, creating character encoding issues.

0.5.1
	- missed the ssl_verify command

0.5.0
	- migrated to the requests library

0.4.13
	- trapping all errors in httplib and urrlib2 ; raising as an NotParsable and sticking the original error into the `raised` attribute.
		this will allow for cleaner error handling
	- we *really* need to move to requests.py

0.4.12
	- created a workaround for sharethis hashbang urls, which urllib2 doesn't like
	- we need to move to requests.py

0.4.11
	- added more relaxed controls for parsing safe files

0.4.10
	- fixed force_parse arg on init
	- added support for more filetypes

0.4.9
	- support for gzip documents that pad with extra data ( spec allows, python module doesn't )
	- ensure proper document format

0.4.8
	- added support for twitter's own og style markup
	- cleaned up the beautifulsoup finds for og data
	- moved 'try' from encapsulating 'for' blocks to encapsulating the inner loop.  this will pull more data out if an error occurs.

0.4.7
	- cleaned up some code

0.4.6
	- realized that some servers return gzip content, despite not advertising that this client accepts that content ; fixed by using some ideas from mark pilgrim's feedparser.  metadata_parser now advertises gzip and zlib, and processes it as needed

0.4.5
	- fixed a bug that prevented toplevel directories from being parsed

0.4.4
	- made redirect/masked/shortened links have better dereferenced url support

0.4.2
	- Wrapped title tag traversal with an AttributeException try block
	- Wrapped canonical tag lookup with a KeyError try block, defaulting to 'href' then 'content'
	- Added support for `url_actual` and `url_info` , which persist the data from the urllib2.urlopen object's `geturl()` and `info()`
	- `get_discrete_url` and `absolute_url` use the underlying url_actual data
	- added support for passing data and headers into urllib2 requests

0.4.1
	Initial Release
